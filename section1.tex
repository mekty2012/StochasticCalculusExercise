\documentclass[a4paper]{article}\usepackage{amsmath,amssymb,amsthm,tikz,hyperref,mathtools,mathrsfs}
\usepackage{bbm}
\usepackage[margin=1cm]{geometry}

\newtheorem{exercise}{Exercise}[section]

\input{commands.tex}

\renewcommand{\theenumi}{(\alph{enumi})}
\makeatletter

\newenvironment{customExercise}[1]
 {\count@\c@exercise
 \global\c@exercise#1 %
   \global\advance\c@exercise\m@ne
   \exercise}
  {\endexercise
  \global\c@exercise\count@}

\title{Martingales in Discrete Time}
\author{Taeyoung Kim}
\date{\today}

\setcounter{section}{1}

\begin{document}
\maketitle

\begin{customExercise}{1}
  Suppose we roll two dice, a red and a green one, and let $X$ be the value on the red die and $Y$ be the value on the green die. 
  Let $Z = XY$. 
  \begin{enumerate}
    \item Let $W = E(Z | X)$. What are the possible values of $W$? Give the distribution of $W$.
    \item Do the same exercise for $U = E(X | Z)$.
    \item Do the same exercise for $V = E(Y | X, Z)$.
  \end{enumerate}
\end{customExercise}
\begin{proof}
  Following is the result of $Z$.

  \begin{tabular}{c|c c c c c c }
    & 1 & 2 & 3 & 4 & 5 & 6 \\
    \hline
    1 & 1 & 2 & 3 & 4 & 5 & 6 \\
    2 & 2 & 4 & 6 & 8 & 10 & 12 \\
    3 & 3 & 6 & 9 & 12 & 15 & 18 \\
    4 & 4 & 8 & 12 & 16 & 20 & 24 \\
    5 & 5 & 10 & 15 & 20 & 25 & 30 \\
    6 & 6 & 12 & 18 & 24 & 30 & 36
  \end{tabular}
  
  \begin{enumerate}
    \item The possible values are $3.5, 7, 10.5, 14, 17.5, 21$. The probability is equal to $1/6$ for all of these values.
    \item 
    \begin{tabular}{c | c | c}
      Value of $Z$ & Value of $E(X | Z)$ & Probability
      \\
      \hline
      1 & 1 & 1/36
      \\
      2 & 1.5 & 2/36
      \\
      3 & 2 & 2/36
      \\
      4 & 7/3 & 3/36
      \\
      5 & 3 & 2/36
      \\
      6 & 3 & 4/36 
      \\
      8 & 3 & 2/36 
      \\
      9 & 3 & 1/36 
      \\
      10 & 3.5 & 2/36
      \\
      12 & 15/4& 4/36 
      \\
      15 & 4 & 2/36
      \\
      16 & 4 & 1/36
      \\
      18 & 4.5 & 2/36 
      \\
      20 & 4.5 & 2/36 
      \\
      24 & 5 & 2/36 
      \\
      25 & 5 & 1/36
      \\
      30 & 5.5 & 2/36
      \\
      36 & 6 & 1/36
    \end{tabular}
    So, the distribution of $U$ is as follows:

    \begin{tabular}{c|c}
      Value & Probability
      \\ \hline
      1 & 1/36
      \\
      1.5 & 2/36
      \\
      2 & 2/36
      \\
      7/3 & 3/36
      \\
      3 & 9/36
      \\
      3.5 & 2/36
      \\
      15/4 & 4/36
      \\
      4 & 3/36
      \\
      4.5 & 4/36
      \\
      5 & 3/36
      \\
      5.5 & 2/36
      \\
      6 & 1/36
    \end{tabular}
    \item Given $X, Z$, $Y$ is determined. So, $V$ has the same distribution as $Y$.
  \end{enumerate}
\end{proof}

\begin{customExercise}{2}
  Suppose we roll two dice, a red and a green one, and let $X$ be the value on the red die and $Y$ be the value on the green die.
  Let $Z = X/Y$.
  \begin{enumerate}
    \item Find $E[(X+2Y)^2 | X]$.
    \item Find $E[(X + 2Y)^2 | X, Z]$.
    \item Find $E[X + 2Y | Z]$.
    \item Let $W = E[Z | X]$. What are the possible values of $W$? Give the distribution of $W$.
  \end{enumerate}
\end{customExercise}
\begin{proof}
  \begin{enumerate}
    \item \begin{align*}
      E[(X+2Y)^2 | X] &= E[X^2 + 4XY + 4Y^2 | X]
      \\
      {} &= X^2 + 4 X \E[Y] + 4 \E[Y^2]
      \\
      {} &= X^2 + 14 X + 182/3.
    \end{align*}
    \item Given $X, Z$, $Y$ is determined. So, $E[(X + 2Y)^2 | X, Z] = (X + 2X/Z)^2$.
    \item We have
    \begin{align*}
      E[X | Z] &= 3.5Z,
      \\
      E[Y | Z] &= 3.5 / Z,
    \end{align*}
    so by the linearity, 
    \[
      E[X + 2Y | Z] = 3.5Z + 7 / Z.
    \]
    \item $E[Z | X] = E[X / Y | X] = X E[1/Y | X] = X E[1/Y] = 49X / 120$. 
  \end{enumerate}
\end{proof}

\begin{customExercise}{3}
  Suppose $X_1, X_2, \ldots$ are independent random variables with
  \[
    P(X_j = 2) = 1 - P(X_j = -1) = \frac{1}{3}.
  \]
  Let $S_n = X_1 + \cdots + X_n$ and let $\cF_n$ denote the information in $X_1, \ldots, X_n$.
  \begin{enumerate}
    \item Find $\E[S_n], \E[S_n^2], \E[S_n^3]$.
    \item If $m < n$, find
    \[
      E[S_n | \cF_m], \E[S_n^2 | \cF_m], \E[S_n^3 | \cF_m].
    \]
    \item If $m < n$, find $E[X_m | S_n]$.
  \end{enumerate}
\end{customExercise}
\begin{proof}
  \begin{enumerate}
    \item Note that $\E[X_j] = 0, \E[X_j^2] = 2, \E[X_j^3] = 2$.
    \begin{align*}
      \E[S_n] &= \sum_{i=1}^n E[X_n] = 0,
      \\
      \E[S_n^2] &= \E\left[ \sum_{i=1}^n X_i^2 + 2 \sum_{i < j} X_i X_j \right] = \sum_{i=1}^n \E[X_i^2] + 2 \sum_{i < j} \E[X_i] \E[X_j] = 2n,
      \\
      \E[S_n^3] &= \E \left[ \sum_{i=1}^n X_i^3 + \sum_{i=1}^n \sum_{j \neq i} (X_i^2 X_j + X_i X_j^2) + \sum_{i=1}^n \sum_{j \neq i} \sum_{k \neq i, j} X_i X_j X_k \right]
      \\
      {} &= \sum_{i=1}^n \E[X_i^3] + \sum_{i=1}^n \sum_{j \neq i} (\E[X_i^2] \E[X_j] + \E[X_i] \E[X_j^2]) + \sum_{i=1}^n \sum_{j \neq i} \sum_{k \neq i, j} \E[X_i] \E[X_j] \E[X_k]
      \\
      {} &= \sum_{i=1}^n \E[X_i^3] = 2n. 
    \end{align*}
    \item Write $S_{m:n} = X_{m+1} + \cdots + X_n$. 
    Then, $S_n = S_{m:n} + S_m$. 
    Since $S_{m:n}$ is independent of $\cF_m$ and $S_m$ is $\cF_m$-measurable, we have
    \begin{align*}
      E[S_n | \cF_m] &= \E[S_{m:n}] + S_m = S_m,
      \\
      E[S_n^2 | \cF_m] &= \E[S_{m:n}^2] + S_m^2 = 2(n-m) + S_m^2,
      \\
      E[S_n^3 | \cF_m] &= \E[S_{m:n}^3] + S_m^3 = 2(n-m) + S_m^3.
    \end{align*}
    \item Suppose that $S_n = k$, then we can see there is exactly $k$ of $X_i$'s that are equal to $2$, and $n-k$ of $X_i$'s that are equal to $-1$.
    This implies that 
    \[
      P(X_m = 2 | S_n = k) = \frac{k}{n}, \quad P(X_m = -1 | S_n = k) = \frac{n-k}{n}.
    \]
    Therefore
    \[
      E[X_m | S_n] = \frac{2 S_n}{n} + \frac{n - S_n}{n} = 1 + \frac{S_n}{n}.
    \]
  \end{enumerate}
\end{proof}

\begin{customExercise}{4}
  Repeat Exercise 1.3 assuming that
  \[
    P(X_j = 3) = 1 - P(X_j = -1) = \frac{1}{2}.
  \]
\end{customExercise}
\begin{proof}
  \begin{enumerate}
    \item Note that $\E[X_j] = 1, \E[X_j^2] = 5, \E[X_j^3] = 13$.
    \begin{align*}
      \E[S_n] &= \sum_{i=1}^n E[X_n] = n,
      \\
      \E[S_n^2] &= \E\left[ \sum_{i=1}^n X_i^2 + 2 \sum_{i < j} X_i X_j \right] = \sum_{i=1}^n \E[X_i^2] + 2 \sum_{i < j} \E[X_i] \E[X_j] = 5 n + n(n-1) = n^2 + 4n,
      \\
      \E[S_n^3] &= \E \left[ \sum_{i=1}^n X_i^3 + 2\sum_{i=1}^n \sum_{j < i} (X_i^2 X_j + X_i X_j^2) + 6\sum_{i=1}^n \sum_{j < i} \sum_{k < i, j} X_i X_j X_k \right]
      \\
      {} &= \sum_{i=1}^n \E[X_i^3] + 2\sum_{i=1}^n \sum_{j < i} (\E[X_i^2] \E[X_j] + \E[X_i] \E[X_j^2]) + 6\sum_{i=1}^n \sum_{j < i} \sum_{k < i, j} \E[X_i] \E[X_j] \E[X_k]
      \\
      {} &= 13 n + 10n(n-1) + n(n-1)(n-2) = n^3 + 7n^2 + 5n. 
    \end{align*}
    \item We can use the same argument as in Exercise 1.3.
    \begin{align*}
      E[S_n | \cF_m] &= S_m,
      \\
      E[S_n^2 | \cF_m] &= S_m^2 + (n-m)^2 + 4(n-m) ,
      \\
      E[S_n^3 | \cF_m] &= S_m^3 + (n-m)^3 + 7(n-m)^2 + 5(n-m).
    \end{align*}
    \item If $S_n = 2k$, then $k$ of $X_i$'s are equal to $3$ and $n-k$ of $X_i$'s are equal to $-1$. 
    This implies that
    \[
      P(X_m = 3 | S_n = 2k) = \frac{k}{n}, \quad P(X_m = -1 | S_n = 2k) = \frac{n-k}{n}.
    \]
    Therefore
    \[
      E[X_m | S_n] = \frac{3 S_n}{2n} - \frac{2n - S_n}{2n} = \frac{2S_n}{n} - 1.
    \]
  \end{enumerate}
\end{proof}

\begin{customExercise}{5}
  Suppose $X_1, X_2, \ldots$ are independent random variables with
  \[
    P(X_j = 1) = P(X_j = -1) = \frac{1}{2}.
  \]
  Let $S_n = X_1 + \cdots + X_n$. Find
  \[
    E(\sin S_n | S_n^2).
  \]
\end{customExercise}
\begin{proof}
  Note that the distribution of $S_n$ is symmetric, i.e., $P(S_n = k) = P(S_n = -k)$ for all $k$.
  This means
  \[
    E(\sin S_n | S_n^2) = \frac{\sin \sqrt{S_n^2} - \sin\sqrt{S_n^2}}{2} = 0. 
  \]
\end{proof}

\begin{customExercise}{6}
  In this exercise, we consider simple, nonsymmetric random walk.
  Suppose $1/2 < 1 < 1$ and $X_1, X_2, \ldots$ are independent random variables with
  \[
    P(X_j = 1) = 1 - P(X_j = -1) = q.
  \]
  Let $S_0 = 0$ and $S_n = X_1 + \cdots + X_n$.
  Let $\cF_n$ denote the information contained in $X_1, \ldots, X_n$.
  \begin{enumerate}
    \item Which of these is $S_n$: martingale, submartingale, supermartingale?
    \item For which values of $r$ is $M_n = S_n - rn$ a martingale?
    \item Let $\theta = (1-q)/q$ and let 
    \[
      M_n = \theta^{S_n}.
    \]
    Show that $M_n$ is a martingale.
    \item Let $a, b$ be positive integers, and 
    \[
      T_{a,b} = \min\{j : S_J = b \text{ or }S_j = -1\}.
    \]
    Use the optional sampling theorem to determine 
    \[
      P(S_{T_{a,b}} = b).
    \]
    \item Let $T_a = T_{a,\infty}$. Find
    \[
      P(T_a < \infty).
    \]
  \end{enumerate}
\end{customExercise}
\begin{proof}
  \begin{enumerate}
    \item We have
    \[
      E[S_n | \cF_{n-1}] = S_{n-1} + E[X_n] = S_{n-1} + 2q-1 > S_{n-1},
    \]
    so it is a submartingale.
    \item As we have seen right before, subtracting $2q-1$ everytime will give us a martingale. 
    So, $r = 2q-1$.
    \item 
    \begin{align*}
      E[M_n | \cF_{n-1}] &= E\left[\theta^{S_n} | \cF_{n-1}\right]
      \\
      {} &= E \left[\theta^{\sum_{i=1}^n X_i} | \cF_{n-1}\right]
      \\
      {} &= E \left[ \prod_{i=1}^n \theta^{X_i} | \cF_{n-1} \right]
      \\
      {} &= \prod_{i=1}^{n-1} \theta^{X_i} E[\theta^{X_n}]
      \\
      {} &= \theta^{S_{n-1}} E[\theta^{X_n}]
      \\
      {} &= \theta^{S_{n-1}} \left( q \cdot \frac{1-q}{q} + (1-q) \cdot \frac{q}{1-q} \right)
      \\
      {} &= \theta^{S_{n-1}}.
    \end{align*}
    \item Let's use the previous example. By the definition of stopping time $T$, we have
    \[
      S_{n \wedge T_{a,b}} \in [-a, b],
    \]
    hence
    \[
      \E[M_{n \wedge T_{a,b}}^2] \le \theta^{-2} < \infty,
    \]
    satisfying the condition of optional sampling theorem 3.
    So,
    \[
      P(T=b) \theta^b + P(T=-a) \theta^{-a} = \E[M_T] = \E[M_0] = 1.
    \]
    Write $P(T=b) = p$, then we have
    \[
      P(T=b) = \frac{\theta^{-a} - 1}{\theta^{-a} - \theta^b}, \quad P(T=a) = \frac{1 - \theta^b}{\theta^{-a} - \theta^b}.
    \]
    (Note that $\theta \in (0, 1)$, therefore $\theta^{-a} > 1$ and $\theta^b < 1$.)
    \item We use similar technique as Example 1.3.3., 
    \[
      P(T_a = \infty) = \lim_{b \to \infty} P(S_{T_{a,b}} = b) = \lim_{b \to \infty} \frac{\theta^{-a} - 1}{\theta^{-a} - \theta^b} = \frac{\theta^{-a} - 1}{\theta^{-a}}.
    \]
    So,
    \[
      P(T_a < \infty) = \frac{1}{\theta^{-a}}.
    \]
  \end{enumerate}
\end{proof}

\begin{customExercise}{7}
  Suppose two people want to play a game in which person A has probability 2/3 of winning. 
  However, the only thing that they have is a fair coin which they can flip as many times as they want.
  They wish to find a method that requires only a finite number of coin flips.
  \begin{enumerate}
    \item Give one method to use the coins to simulate an experiment with probability 2/3 of success. 
    The number of flips needed can be ranodm, but it must be finite with probability one.
    \item Suppose $K < \infty$.
    Explain why there is no method such that with probability one we flip the coin at most $K$ times.
  \end{enumerate}
\end{customExercise}
\begin{proof}
  \begin{enumerate}
    \item Let $S_n = 1 + X_1 + \cdots + X_n$, where $X_i$ is the result of $i$-th coin flip.
    Define the stopping time
    \[
      T = \min\{n : S_n = 0 \text{ or } 3\}
    \]
    then by Example 1.3.1., we have
    \[
      P(S_T = 3) = \frac{1}{3}, \quad P(S_T = 0) = \frac{2}{3}.
    \]
    Note that we have $P(T < \infty) = 1$.

    So, the experiment is as follows:
    \begin{enumerate}
      \item Set initial value to one.
      \item Flip the coin, and add one or subtract one according to the result.
      \item If the value is zero, A wins. If the value is three, B wins.
      \item Otherwise, repeat the second step.
    \end{enumerate}
    \item Given at most $K$ coin flips, we have total $2^K$ equiprobable possible outcomes. 
    There is no way to divide these outcomes into three equiprobable events.
  \end{enumerate}
\end{proof}

\begin{customExercise}{8}
  Repeat the last exercise with 2/3 replaced by $1/\pi$.
\end{customExercise}
\begin{proof}
  We will leverage the series expansion of $1/\pi$,
  \[
    \frac{1}{\pi} = \sum_{n=0}^\infty \frac{((2n)!)^3 (42n + 5)}{(n!)^6 16^{3n+1}} = \sum_{n=0}^\infty \frac{((2n)!)^3 (42n+5)}{(n!)^6 2^{11n+3}} \frac{1}{2^{n+1}},
  \]
  which can be found in \href{https://en.wikipedia.org/wiki/List_of_formulae_involving_%CF%80#Other_infinite_series}{here}. 
  Here, the term
  \[
    \frac{((2n)!)^3 (42n+5)}{(n!)^6 2^{11n+3}} \le \frac{\left( n! \cdot 2^n \right)^6 \cdot (42n+5)}{(n!)^6 2^{11n+3}} = \frac{42n+5}{2^{5n+3}} \in (0, 1).
  \]
  So our experiment is as follows:
  \begin{enumerate}
    \item Let $n = 0$.
    \item We flip the coin. If it is head, we proceed to the next step. If not, we repeat this step with $n \leftarrow n + 1$.
    \item Now, we construct the martingale. Let $S_k = X_1 + \cdots + X_k$, where $X_i$ is the result of $i$-th coin flip.
    \item Define the stopping time as
    \[
      T = \min \{k : S_k = -((2n)!)^3 (42n+5) \text{ or }S_k = (n!)^6 2^{11n+3}-((2n)!)^3 (42n+5)\}.
    \]
    \item If $S_T > 0$, A wins. If $S_T < 0$, B wins. 
  \end{enumerate}
  First, by construction of our martingale and Example 1.3.2.'s reasoning, we have
  \[
    P(S_T > 0) = \frac{((2n)!)^3 (42n+5)}{(n!)^6 2^{11n+3}}.
  \]
  Moreover, by construction of initial step, we have
  \[
    P(n = k) = \frac{1}{2^{k+1}}.
  \]
  Therefore, summing up, we have
  \begin{align*}
    P(\text{A wins}) &= \sum_{k=0}^\infty P(n = k) P(S_T > 0 | n = k)
    \\
    {} &= \sum_{k=0}^\infty \frac{1}{2^{k+1}} \frac{((2k)!)^3 (42k+5)}{(k!)^6 2^{11k+3}}
    \\
    {} &= \frac{1}{\pi}.
  \end{align*}
\end{proof}

\end{document}