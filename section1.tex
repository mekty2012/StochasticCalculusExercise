\documentclass[a4paper]{article}\usepackage{amsmath,amssymb,amsthm,tikz,hyperref,mathtools,mathrsfs}
\usepackage{bbm}
\usepackage[margin=1cm]{geometry}

\newtheorem{exercise}{Exercise}[section]

\input{commands.tex}

\renewcommand{\theenumi}{(\alph{enumi})}
\makeatletter

\newenvironment{customExercise}[1]
 {\count@\c@exercise
 \global\c@exercise#1 %
   \global\advance\c@exercise\m@ne
   \exercise}
  {\endexercise
  \global\c@exercise\count@}

\title{Martingales in Discrete Time}
\author{Taeyoung Kim}
\date{\today}

\setcounter{section}{1}

\begin{document}
\maketitle

\begin{customExercise}{1}
  Suppose we roll two dice, a red and a green one, and let $X$ be the value on the red die and $Y$ be the value on the green die. 
  Let $Z = XY$. 
  \begin{enumerate}
    \item Let $W = E(Z | X)$. What are the possible values of $W$? Give the distribution of $W$.
    \item Do the same exercise for $U = E(X | Z)$.
    \item Do the same exercise for $V = E(Y | X, Z)$.
  \end{enumerate}
\end{customExercise}
\begin{proof}
  Following is the result of $Z$.

  \begin{tabular}{c|c c c c c c }
    & 1 & 2 & 3 & 4 & 5 & 6 \\
    \hline
    1 & 1 & 2 & 3 & 4 & 5 & 6 \\
    2 & 2 & 4 & 6 & 8 & 10 & 12 \\
    3 & 3 & 6 & 9 & 12 & 15 & 18 \\
    4 & 4 & 8 & 12 & 16 & 20 & 24 \\
    5 & 5 & 10 & 15 & 20 & 25 & 30 \\
    6 & 6 & 12 & 18 & 24 & 30 & 36
  \end{tabular}
  
  \begin{enumerate}
    \item The possible values are $3.5, 7, 10.5, 14, 17.5, 21$. The probability is equal to $1/6$ for all of these values.
    \item 
    \begin{tabular}{c | c | c}
      Value of $Z$ & Value of $E(X | Z)$ & Probability
      \\
      \hline
      1 & 1 & 1/36
      \\
      2 & 1.5 & 2/36
      \\
      3 & 2 & 2/36
      \\
      4 & 7/3 & 3/36
      \\
      5 & 3 & 2/36
      \\
      6 & 3 & 4/36 
      \\
      8 & 3 & 2/36 
      \\
      9 & 3 & 1/36 
      \\
      10 & 3.5 & 2/36
      \\
      12 & 15/4& 4/36 
      \\
      15 & 4 & 2/36
      \\
      16 & 4 & 1/36
      \\
      18 & 4.5 & 2/36 
      \\
      20 & 4.5 & 2/36 
      \\
      24 & 5 & 2/36 
      \\
      25 & 5 & 1/36
      \\
      30 & 5.5 & 2/36
      \\
      36 & 6 & 1/36
    \end{tabular}
    So, the distribution of $U$ is as follows:

    \begin{tabular}{c|c}
      Value & Probability
      \\ \hline
      1 & 1/36
      \\
      1.5 & 2/36
      \\
      2 & 2/36
      \\
      7/3 & 3/36
      \\
      3 & 9/36
      \\
      3.5 & 2/36
      \\
      15/4 & 4/36
      \\
      4 & 3/36
      \\
      4.5 & 4/36
      \\
      5 & 3/36
      \\
      5.5 & 2/36
      \\
      6 & 1/36
    \end{tabular}
    \item Given $X, Z$, $Y$ is determined. So, $V$ has the same distribution as $Y$.
  \end{enumerate}
\end{proof}

\begin{customExercise}{2}
  Suppose we roll two dice, a red and a green one, and let $X$ be the value on the red die and $Y$ be the value on the green die.
  Let $Z = X/Y$.
  \begin{enumerate}
    \item Find $E[(X+2Y)^2 | X]$.
    \item Find $E[(X + 2Y)^2 | X, Z]$.
    \item Find $E[X + 2Y | Z]$.
    \item Let $W = E[Z | X]$. What are the possible values of $W$? Give the distribution of $W$.
  \end{enumerate}
\end{customExercise}
\begin{proof}
  \begin{enumerate}
    \item \begin{align*}
      E[(X+2Y)^2 | X] &= E[X^2 + 4XY + 4Y^2 | X]
      \\
      {} &= X^2 + 4 X \E[Y] + 4 \E[Y^2]
      \\
      {} &= X^2 + 14 X + 182/3.
    \end{align*}
    \item Given $X, Z$, $Y$ is determined. So, $E[(X + 2Y)^2 | X, Z] = (X + 2X/Z)^2$.
    \item We have
    \begin{align*}
      E[X | Z] &= 3.5Z,
      \\
      E[Y | Z] &= 3.5 / Z,
    \end{align*}
    so by the linearity, 
    \[
      E[X + 2Y | Z] = 3.5Z + 7 / Z.
    \]
    \item $E[Z | X] = E[X / Y | X] = X E[1/Y | X] = X E[1/Y] = 49X / 120$. 
  \end{enumerate}
\end{proof}

\begin{customExercise}{3}
  Suppose $X_1, X_2, \ldots$ are independent random variables with
  \[
    P(X_j = 2) = 1 - P(X_j = -1) = \frac{1}{3}.
  \]
  Let $S_n = X_1 + \cdots + X_n$ and let $\cF_n$ denote the information in $X_1, \ldots, X_n$.
  \begin{enumerate}
    \item Find $\E[S_n], \E[S_n^2], \E[S_n^3]$.
    \item If $m < n$, find
    \[
      E[S_n | \cF_m], \E[S_n^2 | \cF_m], \E[S_n^3 | \cF_m].
    \]
    \item If $m < n$, find $E[X_m | S_n]$.
  \end{enumerate}
\end{customExercise}
\begin{proof}
  \begin{enumerate}
    \item Note that $\E[X_j] = 0, \E[X_j^2] = 2, \E[X_j^3] = 2$.
    \begin{align*}
      \E[S_n] &= \sum_{i=1}^n E[X_n] = 0,
      \\
      \E[S_n^2] &= \E\left[ \sum_{i=1}^n X_i^2 + 2 \sum_{i < j} X_i X_j \right] = \sum_{i=1}^n \E[X_i^2] + 2 \sum_{i < j} \E[X_i] \E[X_j] = 2n,
      \\
      \E[S_n^3] &= \E \left[ \sum_{i=1}^n X_i^3 + \sum_{i=1}^n \sum_{j \neq i} (X_i^2 X_j + X_i X_j^2) + \sum_{i=1}^n \sum_{j \neq i} \sum_{k \neq i, j} X_i X_j X_k \right]
      \\
      {} &= \sum_{i=1}^n \E[X_i^3] + \sum_{i=1}^n \sum_{j \neq i} (\E[X_i^2] \E[X_j] + \E[X_i] \E[X_j^2]) + \sum_{i=1}^n \sum_{j \neq i} \sum_{k \neq i, j} \E[X_i] \E[X_j] \E[X_k]
      \\
      {} &= \sum_{i=1}^n \E[X_i^3] = 2n. 
    \end{align*}
    \item Write $S_{m:n} = X_{m+1} + \cdots + X_n$. 
    Then, $S_n = S_{m:n} + S_m$. 
    Since $S_{m:n}$ is independent of $\cF_m$ and $S_m$ is $\cF_m$-measurable, we have
    \begin{align*}
      E[S_n | \cF_m] &= \E[S_{m:n}] + S_m = S_m,
      \\
      E[S_n^2 | \cF_m] &= \E[S_{m:n}^2] + S_m^2 = 2(n-m) + S_m^2,
      \\
      E[S_n^3 | \cF_m] &= \E[S_{m:n}^3] + S_m^3 = 2(n-m) + S_m^3.
    \end{align*}
    \item Suppose that $S_n = k$, then we can see there is exactly $k$ of $X_i$'s that are equal to $2$, and $n-k$ of $X_i$'s that are equal to $-1$.
    This implies that 
    \[
      P(X_m = 2 | S_n = k) = \frac{k}{n}, \quad P(X_m = -1 | S_n = k) = \frac{n-k}{n}.
    \]
    Therefore
    \[
      E[X_m | S_n] = \frac{2 S_n}{n} + \frac{n - S_n}{n} = 1 + \frac{S_n}{n}.
    \]
  \end{enumerate}
\end{proof}

\begin{customExercise}{4}
  Repeat Exercise 1.3 assuming that
  \[
    P(X_j = 3) = 1 - P(X_j = -1) = \frac{1}{2}.
  \]
\end{customExercise}
\begin{proof}
  \begin{enumerate}
    \item Note that $\E[X_j] = 1, \E[X_j^2] = 5, \E[X_j^3] = 13$.
    \begin{align*}
      \E[S_n] &= \sum_{i=1}^n E[X_n] = n,
      \\
      \E[S_n^2] &= \E\left[ \sum_{i=1}^n X_i^2 + 2 \sum_{i < j} X_i X_j \right] = \sum_{i=1}^n \E[X_i^2] + 2 \sum_{i < j} \E[X_i] \E[X_j] = 5 n + n(n-1) = n^2 + 4n,
      \\
      \E[S_n^3] &= \E \left[ \sum_{i=1}^n X_i^3 + 2\sum_{i=1}^n \sum_{j < i} (X_i^2 X_j + X_i X_j^2) + 6\sum_{i=1}^n \sum_{j < i} \sum_{k < i, j} X_i X_j X_k \right]
      \\
      {} &= \sum_{i=1}^n \E[X_i^3] + 2\sum_{i=1}^n \sum_{j < i} (\E[X_i^2] \E[X_j] + \E[X_i] \E[X_j^2]) + 6\sum_{i=1}^n \sum_{j < i} \sum_{k < i, j} \E[X_i] \E[X_j] \E[X_k]
      \\
      {} &= 13 n + 10n(n-1) + n(n-1)(n-2) = n^3 + 7n^2 + 5n. 
    \end{align*}
    \item We can use the same argument as in Exercise 1.3.
    \begin{align*}
      E[S_n | \cF_m] &= S_m,
      \\
      E[S_n^2 | \cF_m] &= S_m^2 + (n-m)^2 + 4(n-m) ,
      \\
      E[S_n^3 | \cF_m] &= S_m^3 + (n-m)^3 + 7(n-m)^2 + 5(n-m).
    \end{align*}
    \item If $S_n = 2k$, then $k$ of $X_i$'s are equal to $3$ and $n-k$ of $X_i$'s are equal to $-1$. 
    This implies that
    \[
      P(X_m = 3 | S_n = 2k) = \frac{k}{n}, \quad P(X_m = -1 | S_n = 2k) = \frac{n-k}{n}.
    \]
    Therefore
    \[
      E[X_m | S_n] = \frac{3 S_n}{2n} - \frac{2n - S_n}{2n} = \frac{2S_n}{n} - 1.
    \]
  \end{enumerate}
\end{proof}

\begin{customExercise}{5}
  Suppose $X_1, X_2, \ldots$ are independent random variables with
  \[
    P(X_j = 1) = P(X_j = -1) = \frac{1}{2}.
  \]
  Let $S_n = X_1 + \cdots + X_n$. Find
  \[
    E(\sin S_n | S_n^2).
  \]
\end{customExercise}
\begin{proof}
  Note that the distribution of $S_n$ is symmetric, i.e., $P(S_n = k) = P(S_n = -k)$ for all $k$.
  This means
  \[
    E(\sin S_n | S_n^2) = \frac{\sin \sqrt{S_n^2} - \sin\sqrt{S_n^2}}{2} = 0. 
  \]
\end{proof}

\begin{customExercise}{6}
  In this exercise, we consider simple, nonsymmetric random walk.
  Suppose $1/2 < 1 < 1$ and $X_1, X_2, \ldots$ are independent random variables with
  \[
    P(X_j = 1) = 1 - P(X_j = -1) = q.
  \]
  Let $S_0 = 0$ and $S_n = X_1 + \cdots + X_n$.
  Let $\cF_n$ denote the information contained in $X_1, \ldots, X_n$.
  \begin{enumerate}
    \item Which of these is $S_n$: martingale, submartingale, supermartingale?
    \item For which values of $r$ is $M_n = S_n - rn$ a martingale?
    \item Let $\theta = (1-q)/q$ and let 
    \[
      M_n = \theta^{S_n}.
    \]
    Show that $M_n$ is a martingale.
    \item Let $a, b$ be positive integers, and 
    \[
      T_{a,b} = \min\{j : S_J = b \text{ or }S_j = -1\}.
    \]
    Use the optional sampling theorem to determine 
    \[
      P(S_{T_{a,b}} = b).
    \]
    \item Let $T_a = T_{a,\infty}$. Find
    \[
      P(T_a < \infty).
    \]
  \end{enumerate}
\end{customExercise}
\begin{proof}
  \begin{enumerate}
    \item We have
    \[
      E[S_n | \cF_{n-1}] = S_{n-1} + E[X_n] = S_{n-1} + 2q-1 > S_{n-1},
    \]
    so it is a submartingale.
    \item As we have seen right before, subtracting $2q-1$ every time will give us a martingale. 
    So, $r = 2q-1$.
    \item 
    \begin{align*}
      E[M_n | \cF_{n-1}] &= E\left[\theta^{S_n} | \cF_{n-1}\right]
      \\
      {} &= E \left[\theta^{\sum_{i=1}^n X_i} | \cF_{n-1}\right]
      \\
      {} &= E \left[ \prod_{i=1}^n \theta^{X_i} | \cF_{n-1} \right]
      \\
      {} &= \prod_{i=1}^{n-1} \theta^{X_i} E[\theta^{X_n}]
      \\
      {} &= \theta^{S_{n-1}} E[\theta^{X_n}]
      \\
      {} &= \theta^{S_{n-1}} \left( q \cdot \frac{1-q}{q} + (1-q) \cdot \frac{q}{1-q} \right)
      \\
      {} &= \theta^{S_{n-1}}.
    \end{align*}
    \item Let's use the previous example. By the definition of stopping time $T$, we have
    \[
      S_{n \wedge T_{a,b}} \in [-a, b],
    \]
    hence
    \[
      \E[M_{n \wedge T_{a,b}}^2] \le \theta^{-2} < \infty,
    \]
    satisfying the condition of optional sampling theorem 3.
    So,
    \[
      P(T=b) \theta^b + P(T=-a) \theta^{-a} = \E[M_T] = \E[M_0] = 1.
    \]
    Write $P(T=b) = p$, then we have
    \[
      P(T=b) = \frac{\theta^{-a} - 1}{\theta^{-a} - \theta^b}, \quad P(T=a) = \frac{1 - \theta^b}{\theta^{-a} - \theta^b}.
    \]
    (Note that $\theta \in (0, 1)$, therefore $\theta^{-a} > 1$ and $\theta^b < 1$.)
    \item We use a similar technique as Example 1.3.3., 
    \[
      P(T_a = \infty) = \lim_{b \to \infty} P(S_{T_{a,b}} = b) = \lim_{b \to \infty} \frac{\theta^{-a} - 1}{\theta^{-a} - \theta^b} = \frac{\theta^{-a} - 1}{\theta^{-a}}.
    \]
    So,
    \[
      P(T_a < \infty) = \frac{1}{\theta^{-a}}.
    \]
  \end{enumerate}
\end{proof}

\begin{customExercise}{7}
  Suppose two people want to play a game in which person A has probability 2/3 of winning. 
  However, the only thing that they have is a fair coin which they can flip as many times as they want.
  They wish to find a method that requires only a finite number of coin flips.
  \begin{enumerate}
    \item Give one method to use the coins to simulate an experiment with probability 2/3 of success. 
    The number of flips needed can be ranodm, but it must be finite with probability one.
    \item Suppose $K < \infty$.
    Explain why there is no method such that with probability one we flip the coin at most $K$ times.
  \end{enumerate}
\end{customExercise}
\begin{proof}
  \begin{enumerate}
    \item Let $S_n = 1 + X_1 + \cdots + X_n$, where $X_i$ is the result of $i$-th coin flip.
    Define the stopping time
    \[
      T = \min\{n : S_n = 0 \text{ or } 3\}
    \]
    then by Example 1.3.1., we have
    \[
      P(S_T = 3) = \frac{1}{3}, \quad P(S_T = 0) = \frac{2}{3}.
    \]
    Note that we have $P(T < \infty) = 1$.

    So, the experiment is as follows:
    \begin{enumerate}
      \item Set initial value to one.
      \item Flip the coin, and add one or subtract one according to the result.
      \item If the value is zero, A wins. If the value is three, B wins.
      \item Otherwise, repeat the second step.
    \end{enumerate}
    \item Given at most $K$ coin flips, we have total $2^K$ equiprobable possible outcomes. 
    There is no way to divide these outcomes into three equiprobable events.
  \end{enumerate}
\end{proof}

\begin{customExercise}{8}
  Repeat the last exercise with 2/3 replaced by $1/\pi$.
\end{customExercise}
\begin{proof}
  We will leverage the series expansion of $1/\pi$,
  \[
    \frac{1}{\pi} = \sum_{n=0}^\infty \frac{((2n)!)^3 (42n + 5)}{(n!)^6 16^{3n+1}} = \sum_{n=0}^\infty \frac{((2n)!)^3 (42n+5)}{(n!)^6 2^{11n+3}} \frac{1}{2^{n+1}} ,
  \]
  which can be found in \href{https://en.wikipedia.org/wiki/List_of_formulae_involving_%CF%80#Other_infinite_series}{here}\footnote{\text{We use the convention }0!=1.}. 
  Here, the summands satisfy the following:
  \[
    0 \le \frac{((2n)!)^3 (42n+5)}{(n!)^6 2^{11n+3}} \le \frac{\left( n! \cdot 2^n \right)^6 \cdot (42n+5)}{(n!)^6 2^{11n+3}} = \frac{42n+5}{2^{5n+3}} \le 1.
  \]
  So our experiment is as follows:
  \begin{enumerate}
    \item Let $n = 0$.
    \item We flip the coin. If it is head, we proceed to the next step. If not, we repeat this step with $n \leftarrow n + 1$.
    \item Now, we construct the martingale. Let $S_k = X_1 + \cdots + X_k$, where $X_i$ is the result of $i$-th coin flip.
    \item Define the stopping time as
    \[
      T = \min \{k : S_k = -((2n)!)^3 (42n+5) \text{ or }S_k = (n!)^6 2^{11n+3}-((2n)!)^3 (42n+5)\}.
    \]
    \item If $S_T > 0$, A wins. If $S_T < 0$, B wins. 
  \end{enumerate}
  First, by the construction of our martingale and Example 1.3.2.'s reasoning, we have
  \[
    P(S_T > 0) = \frac{((2n)!)^3 (42n+5)}{(n!)^6 2^{11n+3}} .
  \]
  Moreover, by the construction of the initial step, we have
  \[
    P(n = k) = \frac{1}{2^{k+1}}.
  \]
  Therefore, summing up, we have
  \begin{align*}
    P(\text{A wins}) &= \sum_{k=0}^\infty P(n = k) P(S_T > 0 | n = k)
    \\
    {} &= \sum_{k=0}^\infty  \frac{((2k)!)^3 (42k+5)}{(k!)^6 2^{11k+3}} \frac{1}{2^{k+1}}
    \\
    {} &= \frac{1}{\pi}.
  \end{align*}
\end{proof}


\begin{customExercise}{9}
  Let $X_1, X_2, \ldots$ be independent, identically distributed random variables with 
  \[
    P(X_j = 2) = \frac{1}{3}, \quad P\left(X_j = \frac{1}{2}\right) = \frac{2}{3}.
  \]
  Let $M_0 = 1$ and for $n \ge 1$, $M_n = X_1 X_2 \cdots X_n$.
  \begin{enumerate}
    \item Show that $M_n$ is a matringale.
    \item Explain why $M_n$ satisfies the conditions of the martingale convergence theorem.
    \item Find $M_\infty = \lim_{n \to \infty} M_n$. Explain why $M_\infty = 0$.
    \item Use the optional sampling theorem to determine the probability that $M_n$ ever attains a value as large as $64$.
    \item Does there exist a $C < \infty$ such that $\E[M_n^2] \le C$ for all $n$?
  \end{enumerate}
\end{customExercise}
\begin{proof}
  \begin{enumerate}
    \item We have
    \begin{align*}
      E[M_n | \cF_{n-1}] &= E[X_n X_{n-1} \cdots X_1 | \cF_{n-1}]
      \\
      {} &= E[X_n | \cF_{n-1}] X_{n-1} \cdots X_1
      \\
      {} &= E[X_n] X_{n-1} \cdots X_1
      \\
      {} &= \left( 2 \cdot \frac{1}{3} + \frac{1}{2} \cdot \frac{2}{3} \right) \cdot X_{n-1} \cdots X_1
      \\
      {} &= M_{n-1}.
    \end{align*}
    \item Observe that $M_n$ is always positive, hence $\E|M_n| = \E[M_n] = \E[M_0] = 1$.
    \item Consider $\log_2 M_n$, then by the strong law of large number, we have
    \[
      \frac{1}{n} \log_2 M_n \stackrel{a.s.}{\to} - \frac{1}{3}.
    \]
    For some sequence $a_n$, one can see that $\lim_{n \to \infty} \frac{1}{n} a_n < 0$ implies $\lim_{n \to \infty} a_n = -\infty$.
    Therefore,
    \[
      \lim M_n \stackrel{a.s.}{\to} 0.
    \] 
    \item Since $M_n$ is martingale, we have $\E M_n = \E M_0 = 1$.
    Let $T_a$ be the stopping time such that
    \[
      T_a = \min \{n : M_n = 2^{-a} \text{ or } 64\}.
    \]
    Then we have
    \[
      \E M_{T_a} = 2^{-a} P(M_{T_a} = 2^{-a}) + 64 P(M_{T_a} = 64) = 1.
    \]
    Write $P(M_{T_a} = 64) = p$, then
    \[
      64 p + 2^{-a} (1-p) = 1, \quad p = \frac{1 - 2^{-a}}{64 - 2^{-a}}.
    \]
    Under the limit $a \to \infty$, we have 
    \[
      P(M_{T_\infty} = 64) = \frac{1}{64}.
    \]
    \item Consider the event that only $2$ appears in the sequence. 
    Then, the probability of this event until $n$ is $3^{-n}$, but the value of $M_n^2$ is $4^n$.
    Since $M_n^2 \ge 0$, we have
    \[
      \E M_n^2 \ge 4^n 3^{-n} \to \infty.
    \]
  \end{enumerate}
\end{proof}

\begin{customExercise}{10}
  Let $X_1, X_2, \ldots$ be independent, identically distributed random variables with
  \[
    P(X_j = 1) = q, \quad P(X_j = -1) = 1-q.
  \]
  Let $S_0 = 0$ and $S_n = X_1 + \cdots + X_n$. Let $Y_n = \exp(S_n)$.
  \begin{enumerate}
    \item For which value of $q$ is $Y_n$ a martingale?
    \item For the remaining parts of this exercise assume $q$ takes the value from part 1.
    Explain why $Y_n$ satisfies the conditions of the martingale convergence theorem.
    \item Find $Y_\infty = \lim_n Y_n$. Explain why $Y_\infty = 0$.
    \item Use the optional sampling theorem to determine the probability that $Y_n$ ever attains a value as large as $100$.
    \item Does there exist a $C < \infty$ such that $\E[Y_n^2] \le C$ for all $n$?
  \end{enumerate}
\end{customExercise}
\begin{proof}
  \begin{enumerate}
    \item We have 
    \begin{align*}
      E [Y_{n+1} | \cF_n] &= E \left[ \prod_{i=1}^{n+1} \exp(X_i) | \cF_n\right]
      \\
      {} &= \prod_{i=1}^n \exp(X_i) E[\exp(X_{n+1}) | \cF_n]
      \\
      {} &= \exp(S_n) E[\exp(X_{n+1})]
      \\
      {} &= \exp(S_n) \left( qe + (1-q) \cdot \frac{1}{e}  \right),
    \end{align*}
    so $Y_n$ is a martingale if and only if $qe + (1-q) / e = 1$, which is true when $q = \frac{1}{e+1}$.
    \item Note that $Y_n$ is always positive, hence $\E|Y_n| = \E[Y_n] = \E[Y_0] = 1$.
    \item Consider $\log Y_n$, then by the strong law of large number, we have
    \[
      \lim_{n \to \infty} \frac{1}{n} \log Y_n = \frac{-e+1}{e+1} < 0.
    \]
    By the same reasoning as in Exercise 9, the limit of $Y_n$ is $0$.
    \item Note that we have $e^k \ge 100$ when $k \ge 5$, for integer $k$.
    Let $T_a = \min\{n : Y_n = e^{-a} \text{ or } e^5\}$ be the stopping time.
    Then, by the optional sampling theorem, we have
    \[
      1 = \E Y_0 = \E Y_{T_a} = e^5 P(Y_{T_a} = e^5) + e^{-a} P(Y_{T_a} = e^{-a}).
    \]
    If we write $P(Y_{T_a} = e^5) = p$, then we have
    \[
      e^5 p + e^{-a} (1-p) = 1, \quad p = \frac{1 - e^{-a}}{e^5 - e^{-a}}.
    \]
    Under the limit $a \to \infty$, we have
    \[
      P(Y_{T_\infty} = e^5) = \frac{1}{e^5}.
    \]
    \item Again, consider the event that only $X_j=1$ happens. The probability of this event up to $n$ is $(1+e)^{-n}$, where the value of $Y_n^2$ is $e^{2n}$.
    So we have 
    \[
      \E Y_n^2 \ge e^{2n} (1+e)^{-n} \to \infty.
    \]
  \end{enumerate}
\end{proof}

\begin{customExercise}{11}
  This exercise concerns Polya's urn and has a computing/simulation component. 
  Let us start with one red and one green ball as in the lecture and $M_n$ be the fraction of red balls at the $n$-th stage.
  \begin{enumerate}
    \item Show that the distributions of $M_n$ is uniform on the set 
    \[
      \left\{ \frac{1}{n+1}, \frac{2}{n+1}, \ldots, \frac{n}{n+1} \right\}.
    \]
    \item Write a short program that will simulate this urn. Each time you run the program note the fraction of red balls after 600 draws and after 1200 draws. Compare the two fractions. Then, repeat this twenty times.
  \end{enumerate}
\end{customExercise}
\begin{proof}
  \begin{enumerate}
    \item At time step 1, it is trivially either $1/3$ or $2/3$.
    Now let's suppose that the hypothesis holds at time step $n$, i.e., the distribution of $M_n$ is uniform on the set
    \[
      \left\{ \frac{1}{n+1}, \frac{2}{n+1}, \ldots, \frac{n}{n+1} \right\}.
    \]
    For the two edge cases where 
    \[
      M_{n+1} = \frac{1}{n+2}, \text{ or } M_{n+1} = \frac{n+1}{n+2},
    \]
    we have 
    \[
      P\left( M_{n+1} = \frac{1}{n+2} \right) = P\left( M_n = \frac{1}{n+1} \right) \cdot \frac{n}{n+1} = \frac{1}{n} \cdot \frac{n}{n+1} = \frac{1}{n+1},
    \]
    and similar for $M_{n+1} = \frac{n+1}{n+2}$.
    For the rest of the cases, we have
    \begin{align*}
      P\left( M_{n+1} = \frac{k}{n+2} \right) &= P\left( M_n = \frac{k-1}{n+1} \right) \cdot \frac{k-1}{n+1} + P\left( M_n = \frac{k}{n+1} \right) \cdot \frac{n-k+1}{n+1}
      \\
      {} &= \frac{1}{n+1} \left( \frac{k-1}{n+1} + \frac{n-k+1}{n+1} \right) = \frac{1}{n+1}.
    \end{align*}
    \item See \texttt{polya.ipynb} for the implementation. The code contains the simulation of Polya's urn, the trajectory of this process, and the comparison of the fraction of red balls after 600 and 1200 draws.
  \end{enumerate}
\end{proof}
\end{document}